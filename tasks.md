Hereâ€™s a consolidated, â€œbestâ€ofâ€allâ€ **GenPaper V3 Roadmap**â€”12 atomic, testable tasks grouped into four epics, each drawing on the strongest ideas from every proposal:

---

## ğŸ§° Epic 1: Prompt Library & Modular Pipeline

**TASK 1: Prompt Schema & Loader**

* **What:** Define a JSON schema (`promptSchema.json`) for all paper-type & section prompts; build a `loadPrompts()` util that reads, validates, and types them.
* **Why:** Guarantees consistent, maintainable prompt templates.
* **Done When:**

  * Schema validates sample â€œresearchArticleâ€ & â€œliteratureReview.â€
  * `loadPrompts()` returns typed templates or throws clear errors.
* **Effort:** S

**TASK 2: Section-Specific Prompt Templates**

* **What:** Implement real templates in code for each `paperType` + `sectionKey` (e.g. introduction, literatureReview, methodology), embedding explicit depth cues (â€œcritique,â€ â€œcompare,â€ â€œgaps,â€ â€œstatisticsâ€).
* **Why:** Drives critical analysis rather than shallow summarization.
* **Done When:**

  * Unit tests confirm each template contains its required depth keywords.
* **Effort:** S

**TASK 3: Outline Generation Module**

* **What:** Build `generateOutline(paperType, topic, sourceIds, config)` that uses the outline prompt to return a structured array of `{ sectionKey, title, candidatePaperIds[] }`.
* **Why:** Puts structure in usersâ€™ hands before any drafting.
* **Done When:**

  * Pipeline emits an outline stage with JSONâ€parsable sections.
  * E2E mock verifies correct workflow.
* **Effort:** M

**TASK 4: Section Drafting Module**

* **What:** Build `generateSection(sectionKey, contextChunks, promptTemplate)` to call the AI SDK per section and return `{ content, citations[] }`.
* **Why:** Encapsulates sectionâ€byâ€section generation with its own focused RAG context.
* **Done When:**

  * Integration test with dummy chunks verifies extraction of inline citations.
* **Effort:** M

---

## ğŸ” Epic 2: Smart RAG & Regional Boost

**TASK 5: Paper Ingestion â†’ Region Tagging**

* **What:** Extend your ingestion RPC to detect (or stub) `metadata.region` from venue/affiliation.
* **Why:** Enables truly localized search & context for every user.
* **Done When:**

  * Unit test ensures sample venues like â€œUniv. Lagosâ€ yield `region='Nigeria'`.
* **Effort:** S

### TASK-6: Boost Local Papers in enhancedSearch (universal)

**Purpose:**  
Reorder `enhancedSearch()` results so country-tagged papers matching the userâ€™s `localRegion` appear first.

**Steps/Subtasks:**  
1. In your RAG pipeline, after fetching and filtering papers, read each paperâ€™s `metadata.region`.  
2. Partition the result array into those where `region === localRegion` and the rest.  
3. Concatenate back together: `[localPapersâ€¦, otherPapersâ€¦]`.  
4. Add a unit test: mock three papers with regions `['Brazil','USA','Brazil']`, call with `localRegion='Brazil'`, and assert the two Brazil papers come first in output.

**Acceptance Criteria:**  
- Calling `enhancedSearch(topic, { localRegion: 'Japan' })` yields all `metadata.region === 'Japan'` entries at the front.  
- If no papers match that region, original order is unchanged.

**Effort:** S  
**Dependencies:** TASK-5 (global region detection)


---

## ğŸ”„ Epic 3: Depth, Quality & QA

**TASK 7: Citationâ€Density & Depth Checker**

* **What:** After each section draft, run `checkCitationDensity(content, minPerPara)` and regexâ€scan for your depth cues (â€œcompare,â€ â€œcritique,â€ etc.). If either fails, emit a `review` progress event (and optionally auto-retry with a stronger prompt).
* **Why:** Guarantees both scholarly rigor (citations) and critical depth in every paragraph.
* **Done When:**

  * Unit test: sample text lacking citations or â€œcompareâ€ triggers a review event.
* **Effort:** M

* **Status:** âœ… **COMPLETED** - Citation density checker and depth cue scanner implemented with review event emission

**TASK 8: Few-Shot Examples & Final Polish**

* **What:** For high-stakes paper types (e.g. thesis/dissertation), prepend 1â€“2 gold-standard few-shot examples to the section prompt; then after all sections, run a â€œstitch & polishâ€ prompt to ensure transitions & overall flow.
* **Why:** Leverages real exemplars to elevate style and coherence.
* **Done When:**

  * Prompt loader includes examples when `options.fewShot=true`.
  * Final polish pass merges sections into a fluid narrative.
* **Effort:** M

* **Status:** âœ… **COMPLETED** - Few-shot examples and final polish functionality implemented for high-stakes paper types with comprehensive testing

---

## ğŸ¨ Epic 4: UI, Workflow & Testing

**TASK 9: Paper-Type Selector & Outline Review UI**

* **What:** In your `PaperGenerator` form add a â€œPaper Typeâ€ dropdown (Research Article, Lit Review, Thesis, etc.) and a step where users review/adjust the AI-generated outline before drafting.
* **Why:** Gives users structural control and tailors the backend pipeline accordingly.
* **Done When:**

  * UI dropdown passes `paperType` downstream; outline review step appears.
* **Effort:** S

**TASK 10: Section-Level Progress Bar & Controls**

* **What:** Extend your streaming hook and `<Progress>` UI to show distinct stages: â€œOutline â†’ Introduction â†’ Lit Review â†’ â€¦ â†’ Finalizing.â€ Allow â€œRegenerate Sectionâ€ per block.
* **Why:** Improves transparency and user control.
* **Done When:**

  * Frontend displays labeled progress for each outline section.
  * â€œRegenerateâ€ button triggers only that section.
* **Effort:** M

**TASK 11: Unit & Prompt-Library Tests**

* **What:** Write Jest tests covering: promptâ€schema validation, presence of depth cues, outline structure, section invocation stubbing, citationâ€density checker.
* **Why:** Ensures regressions canâ€™t silently break your core pipeline.
* **Done When:**

  * 100% coverage on `src/prompts` and core generation utilities.
* **Effort:** M

**TASK 12: E2E â€œSmokeâ€ & Quality Gate**

* **What:** Create a Playwright (or Cypress) script to run `/generate`, select â€œLit Review,â€ verify: outline appears, each section returns with â‰¥1 citation/para, final markdown contains all headings and a bibliography. Hook this into CI as a quality gate.
* **Why:** Catches integration issues and validates the full, multi-stage flow automatically.
* **Done When:**

  * CI pipeline runs E2E without errors and asserts structure & citation regex.
* **Effort:** L

---

### **Next Steps**

1. **Triage & Prioritize**: Move these 12 tasks into your backlog and rank by immediate user impact.
2. **Sprint 1**: Kick off with TASK-1 â†’ TASK-4 to get your prompt library and modular pipeline wired.
3. **Sprints 2â€“3**: Layer in localization (TASK-5/6), depth checks (TASK-7), and UI enhancements (TASK-9/10).
4. **Sprint 4**: Harden with few-shot (TASK-8) and comprehensive tests (TASK-11/12).

By following this roadmap, youâ€™ll transform GenPaper into a **flexible, multi-stage â€œPaper Factoryâ€** that produces reviewer-ready literature reviews, research articles, theses, and beyondâ€”grounded in local context, deep critical analysis, and rock-solid quality controls.
